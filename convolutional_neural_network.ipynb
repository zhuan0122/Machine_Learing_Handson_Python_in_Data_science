{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Copy of convolutional_neural_network.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true,
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/zhuan0122/Machine_Learing_Handson_Python_in_Data_science/blob/master/convolutional_neural_network.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3DR-eO17geWu"
      },
      "source": [
        "# Convolutional Neural Network"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EMefrVPCg-60"
      },
      "source": [
        "### Importing the libraries"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bmQ9dppdn_2P"
      },
      "source": [
        "# import the library for processing the images and training NNs \n",
        "import tensorflow as tf\n",
        "from keras.preprocessing.image import ImageDataGenerator"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2hDL7byRAw2N",
        "outputId": "aabbae3d-e6eb-4fd6-bf07-a2c81150e966",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "source": [
        "!python3 --version"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Python 3.6.9\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SBrVjHVcqXTL",
        "outputId": "a712e841-f6c8-4a8a-dfcc-959bf2ab45eb",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "source": [
        "tf.__version__"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'2.3.0'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 3
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oxQxCBWyoGPE"
      },
      "source": [
        "## Part 1 - Data Preprocessing"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MvE-heJNo3GG"
      },
      "source": [
        "### Preprocessing the Training set"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "roAUqgtxobA2"
      },
      "source": [
        "# we need to transform the trainning data to aviod overfitting which means a high accuracy on traninning data set but a relative\n",
        "# accuracy on the test data. It is caused by the lack of the variaty or diversity of the images. so Do transformation includes \n",
        "# geometrical transformation, zoom in and out, horitontal flips and so on which is also called Image augmentation \n",
        "# the tool for transfomation is from keras API. search keras API in google \n",
        "# we can be free to try any other  transformation method to see if the accuaracy is increased or not but here we just copy \n",
        "# the given codes from the API page \n",
        "train_datagen = ImageDataGenerator(\n",
        "        rescale=1./255,\n",
        "        shear_range=0.2,\n",
        "        zoom_range=0.2,\n",
        "        horizontal_flip=True)\n",
        "# apply the transformation to the given data set and from the experience we usually downsize the image pixel to be 64x64\n",
        "training_set = train_datagen.flow_from_directory(\n",
        "        'dataset/training_set',\n",
        "        target_size=(64, 64),\n",
        "        batch_size=32,\n",
        "        class_mode='binary') # we take a binary catogery here \n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mrCMmGw9pHys"
      },
      "source": [
        "### Preprocessing the Test set"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rjaAYqDvsKr0"
      },
      "source": [
        "# test image we need to keep it as original without any transformation but with the same scaling \n",
        "test_datagen = ImageDataGenerator(rescale=1./255)\n",
        "test_set = test_datagen.flow_from_directory(\n",
        "        'dataset/test_set',\n",
        "        target_size=(64,64),\n",
        "        batch_size=32,\n",
        "        class_mode='binary')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "af8O4l90gk7B"
      },
      "source": [
        "## Part 2 - Building the CNN"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ces1gXY2lmoX"
      },
      "source": [
        "### Initialising the CNN"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FhWtvBjSsjG6"
      },
      "source": [
        "# initize the input layer as a sequence of neurons. the total number of neuron would be decided automatically by the input \n",
        "cnn = tf.keras.models.Sequential()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "u5YJj_XMl5LF"
      },
      "source": [
        "### Step 1 - Convolution"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yJz7BlmrtAp6"
      },
      "source": [
        "# the first convolution layers is got by convulzed. the filter is the feature detector matrix, we only give how many filters we would use\n",
        "# in this layer to get the feature maps. each filter would filter or detect different feature. and the filter matrix size 3 means is 3x3 matrix\n",
        "# all activation function except the out put layer would be recitifier. Input_shape is the image shape. Colored images are RGB shape \n",
        "# which is [0~255, 0~255,3], for simple computation, we use 64x64x3. if it is black or white images. then it is [, , 1]\n",
        "# all images are 2D dimentional. \n",
        "cnn.add(tf.keras.layers.Conv2D(filters=32, kernel_size=3, activation='relu', input_shape=[64,64,3]))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tf87FpvxmNOJ"
      },
      "source": [
        "### Step 2 - Pooling"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jVDSI3gpuJD8"
      },
      "source": [
        "# pooling here is max pooling. which means takes the maximum value of each pool. pool size is the 2x2 matrix and stride is 2\n",
        "# see the notebook for details of understanding what does these mean\n",
        "cnn.add(tf.keras.layers.MaxPool2D(pool_size=2, strides=2))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xaTOgD8rm4mU"
      },
      "source": [
        "### Adding a second convolutional layer"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "F4QQij0su3s8"
      },
      "source": [
        "# exactly the same as above but no needs for clarify the input shape of the image here and after convolution, it will follow with \n",
        "# the pooling \n",
        "cnn.add(tf.keras.layers.Conv2D(filters=32, kernel_size=3, activation='relu'))\n",
        "cnn.add(tf.keras.layers.MaxPool2D(pool_size=2, strides=2))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tmiEuvTunKfk"
      },
      "source": [
        "### Step 3 - Flattening"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ktQED786vNcb"
      },
      "source": [
        "cnn.add(tf.keras.layers.Flatten())\n",
        "# flattening the final result of the last pooling and make the data as the sequece of vector "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dAoSECOm203v"
      },
      "source": [
        "### Step 4 - Full Connection"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rNG-WIH9vXla"
      },
      "source": [
        "# dense method here is used to full connect the nerous of this layer to the last layer\n",
        "# this fully connected layer is the first hidden layer of the ANN after the flattening layer which literaly is the input layer of the ANN\n",
        "cnn.add(tf.keras.layers.Dense(units=128, activation='relu'))\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kyzfL2Y6v1xq"
      },
      "source": [
        "# I want to try add second hidden layer here to see if the accuracy is enhanced or not\n",
        "cnn.add(tf.keras.layers.Dense(units=128, activation='relu'))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yTldFvbX28Na"
      },
      "source": [
        "### Step 5 - Output Layer"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VochArKKv_Zc"
      },
      "source": [
        "cnn.add(tf.keras.layers.Dense(units=1, activation='sigmoid'))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "D6XkI90snSDl"
      },
      "source": [
        "## Part 3 - Training the CNN"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vfrFQACEnc6i"
      },
      "source": [
        "### Compiling the CNN"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1n9ai9j6wDqw"
      },
      "source": [
        "# adam is the optimizer to perform stochastic gradien descent to find the lowest cost function and update the weights and filter matrix\n",
        "cnn.compile(optimizer = 'adam', loss = 'binary_crossentropy', metrics = ['accuracy'])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ehS-v3MIpX2h"
      },
      "source": [
        "### Training the CNN on the Training set and evaluating it on the Test set"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mb65MSZMwT5g"
      },
      "source": [
        "# the difference fit here from the ANNs is we also evaluate the NNS on the test test. epochs is the rounds we gonna run NNS on the all \n",
        "# data set, it could be tunned \n",
        "cnn.fit(x = training_set, validation_data = test_set, epochs = 25)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "U3PZasO0006Z"
      },
      "source": [
        "## Part 4 - Making a single prediction"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aqOjWPmrxniF"
      },
      "source": [
        "import numpy as np \n",
        "from keras.preprocessing import image\n",
        "# load the sigle image\n",
        "test_image=image.load_img('dataset/single_prediction/cat_or_dog_1.jpg', target_size = (64, 64))\n",
        "#we need to transfer the input image to the array because the 'predict' method only work with the array as input. not directly 2D image\n",
        "test_image = image.img_to_array(test_image)\n",
        "# and also the NNS are trainning with 32 batch size whihc means each time, NNS will be fed with 32 images as a batch, since we \n",
        "# have 8000 total, so it will has 250 steps for all data finished running. and then we will repeat this 'epoch' times\n",
        "# we need to add a fake dimension for the batch 32 even only one sigle image in this batch but we still need to do this for \n",
        "# the same input to the tarinning set \n",
        "test_image = np.expand_dims(test_image, axis = 0) # 0 means rows  is the first dimension.\n",
        "# first dimension means when we access the image we need to call the first dimention first to get into the first batch and then call the \n",
        "# specific image. whcih also could understand as columns and rows. rows are image list number, and each 32 rows are one bacth and columns \n",
        "# is the images object \n",
        "result = cnn.predict(test_image)\n",
        "# from this contribute we could see 1 is dog or cat.\n",
        "training_set.class_indices\n",
        "# log result\n",
        "if result [0][0] == 1:# first 0 is the batch dimension, means first batch and second zero means the first image predict result\n",
        "  prediction == 'dog'\n",
        "else:\n",
        "  prediction=='cat'\n",
        "\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NyCWhJ8y1wPY"
      },
      "source": [
        "print(prediction)"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}